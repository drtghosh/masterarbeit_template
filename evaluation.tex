\chapter{Evaluation}\label{ch:evaluation}



\section{Experimental Setup}


    \subsection{Datasets}
    A popular synthetic dataset was used to train our proposed models. A synthetic dataset was chosen due to its completeness, which was essential for the methods to work. The problem of surface reconstruction with uncertainty quantification from just partial point clouds is already quite challenging, uncertain, and underdetermined. Further lack of information, such as missing regions in ground truth data, would have made it extremely difficult to learn even the surface reconstruction. Uncertainty quantification would be an added challenge. 
    \newline
    
    The dataset used was generated from the popular synthetic 3D point cloud dataset ShapeNet~\cite{ShapeNet}. The same procedure and data split used in ~\cite{PCN} were followed. The dataset is created from CAD models of 8 different categories of objects: airplane, cabinet, car, chair, lamp, sofa, table, and vessel. There are 30974 models in total, out of which 28794 are used for training. For each model used in training, there is one complete point cloud consisting of 16384 points sampled uniformly on the surfaces defined by the meshes, as well as 8 different partial point clouds with varying point counts corresponding to 8 randomly distributed viewpoints. Due to resource constraints, 2048 points out of 16384 were selected to represent the complete shapes. Farthest point sampling was used to select the points for complete clouds. For partial clouds, a maximum of 1024 points was used for learning unless specified otherwise. Out of the remaining 2000 models, 800 were used for validation and 1200 for testing. The validation and test data were equally distributed across different categories, with 100 samples each for validation and 150 samples each for testing. 


    \subsection{Model Architectures}

        \subsubsection{Point Cloud Completion Networks}
        \textbf{Autoencoder.} 
        To learn to predict the encoding for a given input cloud, an autoencoder (encoder-decoder network) was trained, minimizing the Earth Mover's Distance for a reconstruction task. Standard, variational, and vector quantized variational autoencoders were experimented with. Nevertheless, the experiments were primarily conducted using a standard encoder-decoder network. The encoder network E consists of 4 internal fully connected layers with dimensions 64, 128, 128, and 256, respectively. A residual connection between the input and the final internal layer was also added. The latent dimension of the encoding space was chosen to be 128 in general unless stated otherwise. Rather than just using the coordinates of the point clouds, a positional encoding with 6 different encoding functions was concatenated with the 3D coordinates as an input to the encoder network. The output of the encoder network is then fed to the decoder network. The decoder network D consists of 2 hidden linear layers, each having 256 nodes. Leaky rectified linear units were used as the activation function to allow non-zero gradients for negative values.
        \newline \textbf{Generator.}
        For learning to produce encodings corresponding to complete shapes from the encoding of a partial cloud input, a simple generator network was used. The generator network comprises 2 hidden linear layers with dimensions 256 and 512. For MC dropout or MC DropConnect, the input is just the 128-dimensional encoding of the partial cloud. This is the same for the ensemble of generators, with each generator having the same input. Only for the implicit generative model, a noise vector was concatenated with the encoding before passing it to the generator. The noise dimension was chosen to be 32. The hidden layers retain the same structure for all the models except when dropout or DropConnect is applied. Weights or hidden nodes of the network were dropped with a probability of 0.1 in such methods. Again, leaky rectified linear units were used as the activation function.
        
        \subsubsection{Implicit Representation Autoencoder.}
        An autoencoder (standard or variational) was used to learn implicit representations conditioned on partial cloud and noise. The encoder network E used follows the same construction as the encoder network in point cloud completion. However, the decoder network D$_\text{INR}$ must be restructured, as in this case, the implicit representations are to be predicted through the decoder rather than reconstructed point clouds. The decoder D$_\text{INR}$ contains 4 hidden fully connected layers, each with 512 nodes. The input to the decoder is a point in space concatenated with the encoding of the partial cloud input (computed using the encoder) and some noise. The output for each point is a scalar representing the value of the implicit function at that point. The noise dimension $z$ selected was 16. Originally, it was planned to implement the convolutional feature encoder with a modified PointNet~\cite{PointNet} and SIREN decoder~\cite{SIREN} with FiLM conditioning~\cite{FiLM}, as used in~\cite{NeuralHessian}. However, due to resource and time constraints, a slightly simpler architecture was chosen that had fewer parameters.

        \subsubsection{Feature Mapping for Gaussian Process}
        A Multi-Layer Perceptron (MLP) network was used to learn the feature mapping that produces embeddings from points in space, conditioned on partial encoding, before utilizing the mapped embeddings in a Gaussian process. The trained MLP network F contained 2 fully connected hidden layers, each with 32 nodes. The encoder E used in point cloud completion was used to produce the encodings from the partial point clouds. Therefore, the input to the MLP had a dimension of (3 + 128) = 131. The dimension of the embedding space was set to 16, which corresponds to the output space of the MLP. Leaky rectified linear units were used as the activation function.


    \subsection{Hyperparameters}

        \subsubsection{Point Cloud Completion Networks}
        \textbf{Autoencoder.}
        The initial learning rate was set to $10^{-3}$ using the Adam optimizer ~\cite{Adam} with $\beta_1 = 0.9$ and $\beta_2 = 0.999$. An exponential learning rate scheduler~\cite{ExpLR} with a learning rate decay $\gamma = 0.9995$ was used to dynamically adjust the learning rate during the training of the autoencoder. The experiments were performed on a single GPU or two GPUs with at least 8 GB of memory, using a batch size of 200 for 200 epochs on each subcategory of ShapeNet data or the complete building completion dataset. 
        \newline \textbf{Generator.}
        The fixed learning rate was set to $5\cdot10^{-4}$ using the Adam optimizer~\cite{Adam} with $\beta_1 = 0.5$ and $\beta_2 = 0.999$. To train the generator with dropout or DropConnect, the nodes or weights were dropped with a probability of 0.1. To train an ensemble of generators, an ensemble of 10 was created, each with a different initialization of weights with no bias term. The weights of the linear layers were sampled from a normal distribution with a mean of zero and a standard deviation of 0.02. For the implicit generative models, 20 different encodings were generated from the partial cloud encoding based on 20 different noises, while selecting the one closest to the encoding from the complete cloud during training. A fast nearest neighbour search based on dynamic continuous indexing~\cite{DCIkNN} was used to find the nearest encoding as suggested in~\cite{PCCIMLE}. In Eq~\ref{gen_obj}, $\lambda_{encoding} = 5.0$ and $\lambda_{fidelity} = 6.0$ was fixed. The experiments were performed on a single GPU or two GPUs with at least 8 GB of memory, using a batch size of 10 for 200 epochs on each subcategory of ShapeNet data or the complete building completion dataset.

        \subsubsection{Implicit Representation Autoencoder}
        The initial learning rate was set to $10^{-4}$ using the AMSGrad variant of the Adam optimizer~\cite{AMSGrad} with $\beta_1 = 0.9$ and $\beta_2 = 0.999$. A cosine annealing learning rate scheduler~\cite{CosLR} was used with the learning rate going to a minimum of $10^{-6}$. The latent dimension of the encoding space was chosen to be 256. The hyperparameter $M$ was chosen to be 20. That means 20 different forward passes were performed through the decoder for the points on the surface based on 20 noises while choosing the noise that leads to the best approximation of the surface (minimum $\lambda_{\text {manifold }}$) during training. The selected noise is then used to predict the implicit function values for other points (non-manifold and near). In Eq~\ref{non_manifold_sdf}, $\alpha$ was set to 100. In Eq~\ref{Hessian3}, it was fixed that $\lambda_{\text {manifold }} = 7\cdot 10^3, \lambda_{\text {non-manifold }} = 6\cdot 10^2, \lambda_{\text {Eikonal }} = 50, \lambda_{\text{Hessian }} = 3$, and $\lambda_{\text {reg }} = 1$. Following~\cite{NeuralHessian}, during the first $10\%$ iterations, $\tau = 1$ was fixed, then it was linearly decreased to $0.000\overline{3}$ during the $10\%$ to $20\%$ iterations, and finally decreased to $0$ at the termination. The experiments were conducted on two GPUs with at least 8 GB of memory, using a batch size of 1 for 200 epochs on each subcategory of the ShapeNet dataset. 

        \subsubsection{Feature Mapping for Gaussian Process}
        The fixed learning rate was set to $10^{-3}$ using the Adam optimizer ~\cite{Adam} with $\beta_1 = 0.9$ and $\beta_2 = 0.999$. For triplet loss, the margin was set to a value of 2. The experiments were performed on two GPUs with at least 8 GB of memory, with a batch size of 256 for 2000 epochs on each subcategory of the ShapeNet dataset. The number of positive and negative pairs created for each point cloud was 8192. The loss within each batch was accumulated with a batch size of 2048.
        


\section{Evaluation of Point Cloud Completion-based Empirical Methods}\label{pccComp}


    \subsection{Qualitative Comparison}\label{quali}

        \subsubsection{Comparison Between Different Models}
        A qualitative comparison between the uncertainty maps produced by different methods described in Section~\ref{euqpcc} is shown in Figure~\ref{fig:airplane} for the airplane subcategory of ShapeNet data and in Figure~\ref{fig:table} for the table subcategory. The representative examples were chosen randomly from the test data. For each input, 10 different completions were produced, and uncertainty maps were computed using linear assignment matching between the generated clouds.
        \begin{figure}[htb]
          \centering
          \begin{subfigure}[t]{\dimexpr0.315\textwidth+20pt\relax}
            \makebox[20pt]{\raisebox{30pt}{\rotatebox[origin=c]{90}{\small Input}}}%
            \includegraphics[width=\dimexpr\linewidth-20pt\relax]{figures/part_ap1.png}
            \makebox[20pt]{\raisebox{30pt}{\rotatebox[origin=c]{90}{\small DropCon}}}%
            \includegraphics[width=\dimexpr\linewidth-20pt\relax]{figures/dc_lin_ap1.png}
            \makebox[20pt]{\raisebox{30pt}{\rotatebox[origin=c]{90}{\small Dropout}}}%
            \includegraphics[width=\dimexpr\linewidth-20pt\relax]{figures/do_lin_ap1.png}
            \makebox[20pt]{\raisebox{30pt}{\rotatebox[origin=c]{90}{\small Ensemble}}}%
            \includegraphics[width=\dimexpr\linewidth-20pt\relax]{figures/ens_lin_ap1.png}
            \makebox[20pt]{\raisebox{30pt}{\rotatebox[origin=c]{90}{\small Implicit}}}%
            \includegraphics[width=\dimexpr\linewidth-20pt\relax]{figures/iml_lin_ap1.png}
            \makebox[20pt]{\raisebox{30pt}{\rotatebox[origin=c]{90}{\small GT}}}%
            \includegraphics[width=\dimexpr\linewidth-20pt\relax]{figures/com_ap1.png}
            \caption{Airplane 1}
          \end{subfigure}\hfill
          \begin{subfigure}[t]{0.315\textwidth}
            \includegraphics[width=\textwidth]{figures/part_ap2.png}
            \includegraphics[width=\textwidth]{figures/dc_lin_ap2.png}
            \includegraphics[width=\textwidth]{figures/do_lin_ap2.png}
            \includegraphics[width=\textwidth]{figures/ens_lin_ap2.png}
            \includegraphics[width=\textwidth]{figures/iml_lin_ap2.png}
            \includegraphics[width=\textwidth]{figures/com_ap2.png}
            \caption{Airplane 2}
          \end{subfigure}\hfill
          \begin{subfigure}[t]{0.315\textwidth}
            \includegraphics[width=\textwidth]{figures/part_ap3.png}
            \includegraphics[width=\textwidth]{figures/dc_lin_ap3.png}
            \includegraphics[width=\textwidth]{figures/do_lin_ap3.png}
            \includegraphics[width=\textwidth]{figures/ens_lin_ap3.png}
            \includegraphics[width=\textwidth]{figures/iml_lin_ap3.png}
            \includegraphics[width=\textwidth]{figures/com_ap3.png}
            \caption{Airplane 3}
          \end{subfigure}
          \caption{Qualitative comparison between different empirical uncertainty quantification methods for point cloud completion on the airplane subcategory of ShapeNet data. Input refers to the partial point cloud, and GT refers to the ground truth complete cloud. DropCon, Dropout, Ensemble, and Implicit refer to the uncertainty maps output by the DropConnect, dropout, deep ensemble, and implicit generative model-based methods, respectively.}
          \label{fig:airplane}
        \end{figure}
        \newline
        
        As seen in Figure~\ref{fig:airplane}, the DropConnect-based model was not able to learn to generate shapes consistent with the ground truth. Such behaviour can be attributed to either suboptimal training or the fact that each weight in the network is important in predicting the encoding for generated shapes, and dropping them in inference time affects the quality. The other models generated consistent, completed shapes but differed in terms of diversity and fidelity to the partial cloud. The ensemble of generators and dropout-based models produced completed shapes that were only diverse in the geometrically complex regions and overall consistent with the partial point clouds. The dropout-based models generated a bit more diverse shapes compared to an ensemble of generators. The generative implicit models produced the most diverse completions with more uncertainty in both geometrically complex and incomplete (with respect to partial cloud) regions. It also captured that certain regions are expected not to vary significantly, regardless of the sparsity in the input cloud. However, due to the variety of completions, the average completed shape deviated slightly from the partial cloud. A similar pattern can be seen in the results of the table subcategory shown in Figure~\ref{fig:table}.
        
        \begin{figure}[htb]
          \centering
          \begin{subfigure}[t]{\dimexpr0.315\textwidth+20pt\relax}
            \makebox[20pt]{\raisebox{30pt}{\rotatebox[origin=c]{90}{\small Input}}}%
            \includegraphics[width=\dimexpr\linewidth-20pt\relax]{figures/part_t1.png}
            \makebox[20pt]{\raisebox{30pt}{\rotatebox[origin=c]{90}{\small DropCon}}}%
            \includegraphics[width=\dimexpr\linewidth-20pt\relax]{figures/dc_lin_t1.png}
            \makebox[20pt]{\raisebox{30pt}{\rotatebox[origin=c]{90}{\small Dropout}}}%
            \includegraphics[width=\dimexpr\linewidth-20pt\relax]{figures/do_lin_t1.png}
            \makebox[20pt]{\raisebox{30pt}{\rotatebox[origin=c]{90}{\small Ensemble}}}%
            \includegraphics[width=\dimexpr\linewidth-20pt\relax]{figures/ens_lin_t1.png}
            \makebox[20pt]{\raisebox{30pt}{\rotatebox[origin=c]{90}{\small Implicit}}}%
            \includegraphics[width=\dimexpr\linewidth-20pt\relax]{figures/iml_lin_t1.png}
            \makebox[20pt]{\raisebox{30pt}{\rotatebox[origin=c]{90}{\small GT}}}%
            \includegraphics[width=\dimexpr\linewidth-20pt\relax]{figures/com_t1.png}
            \caption{Table 1}
          \end{subfigure}\hfill
          \begin{subfigure}[t]{0.315\textwidth}
            \includegraphics[width=\textwidth]{figures/part_t2.png}
            \includegraphics[width=\textwidth]{figures/dc_lin_t2.png}
            \includegraphics[width=\textwidth]{figures/do_lin_t2.png}
            \includegraphics[width=\textwidth]{figures/ens_lin_t2.png}
            \includegraphics[width=\textwidth]{figures/iml_lin_t2.png}
            \includegraphics[width=\textwidth]{figures/com_t2.png}
            \caption{Table 2}
          \end{subfigure}\hfill
          \begin{subfigure}[t]{0.315\textwidth}
            \includegraphics[width=\textwidth]{figures/part_t3.png}
            \includegraphics[width=\textwidth]{figures/dc_lin_t3.png}
            \includegraphics[width=\textwidth]{figures/do_lin_t3.png}
            \includegraphics[width=\textwidth]{figures/ens_lin_t3.png}
            \includegraphics[width=\textwidth]{figures/iml_lin_t3.png}
            \includegraphics[width=\textwidth]{figures/com_t3.png}
            \caption{Table 3}
          \end{subfigure}
          \caption{Qualitative comparison between different empirical uncertainty quantification methods for point cloud completion on the table subcategory of ShapeNet data. Input refers to the partial point cloud, and GT refers to the ground truth complete cloud. DropCon, Dropout, Ensemble, and Implicit refer to the uncertainty maps output by the DropConnect, dropout, deep ensemble, and implicit generative model-based methods, respectively.}
          \label{fig:table}
        \end{figure}

        \subsubsection{Effect of Number of Generated Samples}
        A comparison between the uncertainty maps produced by different numbers of generated clouds is shown in Figure~\ref{fig:zairplanezz} for a randomly selected example from the airplane subcategory of ShapeNet data. For each input, 5, 10, 25, and 50 different completions were produced for three methods, and uncertainty maps were computed using linear assignment matching between the generated clouds. Ensemble-based methods were ignored due to the computational cost and the memory required to store the models.
        \begin{figure}[htb]
          \centering
          \begin{subfigure}[t]{\dimexpr0.315\textwidth+20pt\relax}
            \makebox[20pt]{\raisebox{30pt}{\rotatebox[origin=c]{90}{\small Input}}}%
            \includegraphics[width=\dimexpr\linewidth-20pt\relax]{figures/part_ap.png}
            \makebox[20pt]{\raisebox{30pt}{\rotatebox[origin=c]{90}{\small $M$=5}}}%
            \includegraphics[width=\dimexpr\linewidth-20pt\relax]{figures/5z_ap_mcdc.png}
            \makebox[20pt]{\raisebox{30pt}{\rotatebox[origin=c]{90}{\small $M$=10}}}%
            \includegraphics[width=\dimexpr\linewidth-20pt\relax]{figures/10z_ap_mcdc.png}
            \makebox[20pt]{\raisebox{30pt}{\rotatebox[origin=c]{90}{\small $M$=25}}}%
            \includegraphics[width=\dimexpr\linewidth-20pt\relax]{figures/25z_ap_mcdc.png}
            \makebox[20pt]{\raisebox{30pt}{\rotatebox[origin=c]{90}{\small $M$=50}}}%
            \includegraphics[width=\dimexpr\linewidth-20pt\relax]{figures/50z_ap_mcdc.png}
            \makebox[20pt]{\raisebox{30pt}{\rotatebox[origin=c]{90}{\small GT}}}%
            \includegraphics[width=\dimexpr\linewidth-20pt\relax]{figures/comp_ap.png}
            \caption{MC DropConnect}
          \end{subfigure}\hfill
          \begin{subfigure}[t]{0.315\textwidth}
            \includegraphics[width=\textwidth]{figures/part_ap.png}
            \includegraphics[width=\textwidth]{figures/5z_ap_mcdo.png}
            \includegraphics[width=\textwidth]{figures/10z_ap_mcdo.png}
            \includegraphics[width=\textwidth]{figures/25z_ap_mcdo.png}
            \includegraphics[width=\textwidth]{figures/50z_ap_mcdo.png}
            \includegraphics[width=\textwidth]{figures/comp_ap.png}
            \caption{MC Dropout}
          \end{subfigure}\hfill
          \begin{subfigure}[t]{0.315\textwidth}
            \includegraphics[width=\textwidth]{figures/part_ap.png}
            \includegraphics[width=\textwidth]{figures/5z_ap_imle.png}
            \includegraphics[width=\textwidth]{figures/10z_ap_imle.png}
            \includegraphics[width=\textwidth]{figures/25z_ap_imle.png}
            \includegraphics[width=\textwidth]{figures/50z_ap_imle.png}
            \includegraphics[width=\textwidth]{figures/comp_ap.png}
            \caption{Implicit Generation}
          \end{subfigure}
          \caption{Qualitative comparison based on the number of generated clouds used to compute the uncertainty map for different empirical uncertainty quantification methods for point cloud completion on the airplane subcategory of ShapeNet data. Here, $M$ denotes the number of completed clouds generated at inference time. Input refers to the partial point cloud, and GT refers to the ground truth complete cloud.}
          \label{fig:zairplanezz}
        \end{figure}
        \newline

        As seen in Figure~\ref{fig:zairplanezz}, the DropConnect-based model again generated shapes inconsistent with the ground truth. However, across all the models, a common pattern can be observed in the uncertainty map for different sample sizes ($M$). As the numbers of clouds generated were increased, the regions with simpler geometry became less varied. However, the complex geometric regions in the produced clouds became more diverse as more samples were generated. Unfortunately, that resulted in undesired spurious geometries. On the other hand, if too few generated clouds are used to create the uncertainty map, the diversity is not enough. A trade-off can be found for $M$ = 10, which is the value used in all our other experiments, unless stated otherwise.

        \subsubsection{Effect of Partial Data Availability}
        A comparison between the uncertainty maps produced by implicit generation for different sizes of an input partial point cloud is shown in Figure~\ref{fig:partialpercentchair} for a randomly selected example from the chair subcategory of the ShapeNet dataset. For input sizes ($N_P$) of 128, 256, 512, and 1024, 10 different completions were produced, and uncertainty maps were computed using linear assignment matching between the generated clouds.
        \begin{figure}[htb]
          \centering
          \begin{subfigure}[t]{\dimexpr0.315\textwidth+20pt\relax}
            \makebox[20pt]{\raisebox{30pt}{\rotatebox[origin=c]{90}{\small $N_P=128$}}}%
            \includegraphics[width=\dimexpr\linewidth-20pt\relax]{figures/percent/128_part.png}
            \makebox[20pt]{\raisebox{30pt}{\rotatebox[origin=c]{90}{\small $N_P=256$}}}%
            \includegraphics[width=\dimexpr\linewidth-20pt\relax]{figures/percent/256_part.png}
            \makebox[20pt]{\raisebox{30pt}{\rotatebox[origin=c]{90}{\small $N_P=512$}}}%
            \includegraphics[width=\dimexpr\linewidth-20pt\relax]{figures/percent/512_part.png}
            \makebox[20pt]{\raisebox{30pt}{\rotatebox[origin=c]{90}{\small $N_P=1024$}}}%
            \includegraphics[width=\dimexpr\linewidth-20pt\relax]{figures/percent/1024_part.png}
            \caption{Partial cloud}\label{fig:partialpercentchair1}
          \end{subfigure}\hfill
          \begin{subfigure}[t]{0.315\textwidth}
            \includegraphics[width=\textwidth]{figures/percent/128_umap_edited.png}
            \includegraphics[width=\textwidth]{figures/percent/256_umap_edited.png}
            \includegraphics[width=\textwidth]{figures/percent/512_umap_edited.png}
            \includegraphics[width=\textwidth]{figures/percent/1024_umap_edited.png}
            \caption{Uncertainty map}\label{fig:partialpercentchair2}
          \end{subfigure}\hfill
          \begin{subfigure}[t]{0.315\textwidth}
            \includegraphics[width=\textwidth]{figures/percent/chair_comp.png}
            \includegraphics[width=\textwidth]{figures/percent/chair_comp.png}
            \includegraphics[width=\textwidth]{figures/percent/chair_comp.png}
            \includegraphics[width=\textwidth]{figures/percent/chair_comp.png}
            \caption{Ground truth}\label{fig:partialpercentchair3}
          \end{subfigure}
          \caption{Qualitative comparison of uncertainty maps predicted using the implicit generative model based on the size of input partial point cloud for completion on the chair subcategory of ShapeNet data.}
          \label{fig:partialpercentchair}
        \end{figure}
        \newline

        As observed in Figure~\ref{fig:partialpercentchair2}, as the number of points in the partial cloud increases, the uncertainty decreases in the regions where more data are available. This is evident from the region in the chair encircled by the red dotted ellipse. Moreover, the predicted shape becomes more similar to the ground truth as more points in the input cloud are available (see the region encircled by the white dotted ellipse).

        \subsubsection{Index-wise versus Linear Assignment-based Matching}
        A comparison between the two possible ways uncertainty maps can be produced from the generated complete clouds is shown in Figure~\ref{fig:matchingwar} for randomly selected examples from the airplane or table subcategory of the ShapeNet dataset. For each input partial cloud, 10 different completions were produced, and uncertainty maps were computed either by using the entries in the same index from the generated clouds or by using linear assignment matching between the generated clouds. The uncertainty maps shown in Figure~\ref{fig:matchingwar1} were produced from implicit generation, in Figure~\ref{fig:matchingwar2} from the dropout-based method, and in Figure~\ref{fig:matchingwar3} from an ensemble of generators.
        \begin{figure}[htb]
          \centering
          \begin{subfigure}[t]{\dimexpr0.315\textwidth+20pt\relax}
            \makebox[20pt]{\raisebox{30pt}{\rotatebox[origin=c]{90}{\small Input}}}%
            \includegraphics[width=\dimexpr\linewidth-20pt\relax]{figures/part_ap1.png}
            \makebox[20pt]{\raisebox{30pt}{\rotatebox[origin=c]{90}{\small Indexed}}}%
            \includegraphics[width=\dimexpr\linewidth-20pt\relax]{figures/iml_ind_ap1.png}
            \makebox[20pt]{\raisebox{30pt}{\rotatebox[origin=c]{90}{\small Assigned}}}%
            \includegraphics[width=\dimexpr\linewidth-20pt\relax]{figures/iml_lin_ap1.png}
            \makebox[20pt]{\raisebox{30pt}{\rotatebox[origin=c]{90}{\small GT}}}%
            \includegraphics[width=\dimexpr\linewidth-20pt\relax]{figures/com_ap1.png}
            \caption{Implicit}\label{fig:matchingwar1}
          \end{subfigure}\hfill
          \begin{subfigure}[t]{0.315\textwidth}
            \includegraphics[width=\textwidth]{figures/part_t1.png}
            \includegraphics[width=\textwidth]{figures/do_ind_t1.png}
            \includegraphics[width=\textwidth]{figures/do_lin_t1.png}
            \includegraphics[width=\textwidth]{figures/com_t1.png}
            \caption{Dropout}\label{fig:matchingwar2}
          \end{subfigure}\hfill
          \begin{subfigure}[t]{0.315\textwidth}
            \includegraphics[width=\textwidth]{figures/part_ap3.png}
            \includegraphics[width=\textwidth]{figures/ens_ind_ap3.png}
            \includegraphics[width=\textwidth]{figures/ens_lin_ap3.png}
            \includegraphics[width=\textwidth]{figures/com_ap3.png}
            \caption{Ensemble}\label{fig:matchingwar3}
          \end{subfigure}
          \caption{Qualitative comparison of uncertainty maps produced by index-wise computation (Indexed) or linear assignment matching (Assigned) for completion on different subcategories of ShapeNet data and for different models used.}
          \label{fig:matchingwar}
        \end{figure}
        \newline

        From the Figure~\ref{fig:matchingwar}, it is evident that the linear assignment matching produces more plausible uncertainty maps, which is expected. For index-wise computation, regions with simpler geometry often remain more uncertain, which is undesirable. Linear assignment matching fixed such issues. Moreover, even in regions where input data is available, naive uncertainty maps can show unnecessarily high uncertainty due to incorrect matching, whereas linear assignment matching addresses such occurrences correctly.

    \subsection{Quantitative Comparison}
    
        \subsubsection{Uncertainty Map}
        A quantitative comparison between the uncertainty maps produced by different methods described in Section~\ref{euqpcc} is shown in Figure~\ref{fig:normstdairplane} for the airplane subcategory of the ShapeNet dataset. The norms of the standard deviations in the uncertainty map across different test inputs are compared based on specific useful metrics. For each input, 10 different completions were produced, and uncertainty maps were computed using linear assignment matching between the generated clouds. Within each uncertainty map, the standard deviation norms at each point were calculated and stored. In the first column of Figure~\ref{fig:normstdairplane}, histograms of all the norms combined across all test inputs are plotted for the different models implemented. For the second column of Figure~\ref{fig:normstdairplane}, only the maximum standard deviation norm for each test input is collected, and the collection of maximum norms across the test inputs is plotted in histograms for the different models. Finally, for the third column of Figure~\ref{fig:normstdairplane}, the average of standard deviation norms within the uncertainty map for each test input is calculated, and the collection of average norms across the test inputs is plotted in histograms.
        \begin{figure}[htb]
          \centering
          \begin{subfigure}[t]{\dimexpr0.315\textwidth+20pt\relax}
            \makebox[20pt]{\raisebox{60pt}{\rotatebox[origin=c]{90}{\small DropCon}}}%
            \includegraphics[width=\dimexpr\linewidth-20pt\relax]{figures/dropcon/matched_all_std_norms.png}
            \makebox[20pt]{\raisebox{60pt}{\rotatebox[origin=c]{90}{\small Dropout}}}%
            \includegraphics[width=\dimexpr\linewidth-20pt\relax]{figures/dropout/matched_all_std_norms.png}
            \makebox[20pt]{\raisebox{60pt}{\rotatebox[origin=c]{90}{\small Ensemble}}}%
            \includegraphics[width=\dimexpr\linewidth-20pt\relax]{figures/ensemble/matched_all_std_norms.png}
            \makebox[20pt]{\raisebox{60pt}{\rotatebox[origin=c]{90}{\small Implicit}}}%
            \includegraphics[width=\dimexpr\linewidth-20pt\relax]{figures/imle/matched_all_std_norms.png}
            \caption{All norms}\label{fig:normstdairplane1}
          \end{subfigure}\hfill
          \begin{subfigure}[t]{0.315\textwidth}
            \includegraphics[width=\textwidth]{figures/dropcon/matched_max_std_norms.png}
            \includegraphics[width=\textwidth]{figures/dropout/matched_max_std_norms.png}
            \includegraphics[width=\textwidth]{figures/ensemble/matched_max_std_norms.png}
            \includegraphics[width=\textwidth]{figures/imle/matched_max_std_norms.png}
            \caption{Maximum norms}\label{fig:normstdairplane2}
          \end{subfigure}\hfill
          \begin{subfigure}[t]{0.315\textwidth}
            \includegraphics[width=\textwidth]{figures/dropcon/matched_avg_std_norms.png}
            \includegraphics[width=\textwidth]{figures/dropout/matched_avg_std_norms.png}
            \includegraphics[width=\textwidth]{figures/ensemble/matched_avg_std_norms.png}
            \includegraphics[width=\textwidth]{figures/imle/matched_avg_std_norms.png}
            \caption{Average norms}\label{fig:normstdairplane3}
          \end{subfigure}
          \caption{Quantitative comparison based on norms of the standard deviations in the uncertainty maps between different empirical uncertainty quantification methods for point cloud completion on the airplane subcategory of ShapeNet data.}
          \label{fig:normstdairplane}
        \end{figure}
        \newline

        As observed in Figure~\ref{fig:normstdairplane1}, most of the standard deviation norms across all points and all test data remain within the range of 0 to 0.025 for DropConnect-based uncertainty quantification. For both dropout-based and ensemble-based uncertainties, the standard deviation norms mostly lie between 0 and 0.05. In contrast, for implicit generation, the upper range, which contains most norms, extends to 0.1, with a wider tail on the right-hand side of the histogram. These numbers confirm the visual observations presented and discussed in Subsection~\ref{quali}, which show that implicit generation produced the most diverse completions, while DropConnect produced the least diverse completions. The dropout and ensemble models, meanwhile, fall between the implicit generation and DropConnect, with quite similar diversity in their generated complete clouds.
        \\
        When comparing the distribution of maximum standard deviation norms per test data (see Figure~\ref{fig:normstdairplane2}), a similar conclusion can be drawn as above. While the peak of the distribution approximated from the histograms lies around 0.1 for DropConnect, dropout, and ensemble-based methods, the peak is around 0.18 for implicit generation, indicating higher standard deviation across test data.
        \\
        A comparison of the average norms of uncertainty maps per test data (see Figure~\ref{fig:normstdairplane3}) shows an identical pattern to the overall collection of norms. The average norms approximately range from 0.008 to 0.02  for DropConnect, 0.005 to 0.035 for dropout, 0.005 to 0.037 for ensemble, and 0.01 to 0.06 for implicit generation. Altogether, it can be concluded that implicit generative models are capable of producing the most diverse completed shapes from partial inputs out of the methods implemented in this work.
        
        \subsubsection{Shape Approximation Consistency}
        Although the main focus of this work has been on uncertainty quantification for surface reconstruction from partial clouds, it is also essential that the reconstructed surface is consistent with the partial clouds and close to the possible complete shapes. 
        \newline
        
        To quantify the consistency or fidelity of the generated clouds with the partial cloud inputs, the unidirectional Hausdorff distance given by Eq~\ref{fidelity_loss} is computed between the partial cloud and each of the 10 generated clouds. In the first column of Figure~\ref{fig:udhsairplane}, histograms of all the values of unidirectional Hausdorff distance combined across all test inputs from the airplane subcategory of the ShapeNet dataset are plotted for the different models. For the second column, the average of the unidirectional Hausdorff distances between each partial cloud and the corresponding generated complete clouds was computed, and a histogram was plotted for average distances from the individual test inputs.

        \begin{figure}[htb]
          \centering
          \begin{subfigure}[t]{\dimexpr0.36\textwidth+20pt\relax}
            \makebox[20pt]{\raisebox{60pt}{\rotatebox[origin=c]{90}{\small DropCon}}}%
            \includegraphics[width=\dimexpr\linewidth-20pt\relax]{figures/dropcon/all_udhs.png}
            \makebox[20pt]{\raisebox{60pt}{\rotatebox[origin=c]{90}{\small Dropout}}}%
            \includegraphics[width=\dimexpr\linewidth-20pt\relax]{figures/dropout/all_udhs.png}
            \makebox[20pt]{\raisebox{60pt}{\rotatebox[origin=c]{90}{\small Ensemble}}}%
            \includegraphics[width=\dimexpr\linewidth-20pt\relax]{figures/ensemble/all_udhs.png}
            \makebox[20pt]{\raisebox{60pt}{\rotatebox[origin=c]{90}{\small Implicit}}}%
            \includegraphics[width=\dimexpr\linewidth-20pt\relax]{figures/imle/all_udhs.png}
            \caption{All distances}\label{fig:udhsairplane1}
          \end{subfigure}\hfill
          \begin{subfigure}[t]{0.36\textwidth}
            \includegraphics[width=\textwidth]{figures/dropcon/matched_mean_udhs.png}
            \includegraphics[width=\textwidth]{figures/dropout/matched_mean_udhs.png}
            \includegraphics[width=\textwidth]{figures/ensemble/matched_mean_udhs.png}
            \includegraphics[width=\textwidth]{figures/imle/matched_mean_udhs.png}
            \caption{Average distances}\label{fig:udhsairplane2}
          \end{subfigure}
          \caption{Quantitative comparison of fidelity of generated clouds with partial inputs between different empirical uncertainty quantification methods for point cloud completion on the airplane subcategory of ShapeNet data using unidirectional Hausdorff distance.}
          \label{fig:udhsairplane}
        \end{figure}

        As observed in Figure~\label{fig:udhsairplane1}, most of the unidirectional Hausdorff distances are below 1.25 for all methods except DropConnect. For DropConnect, the distribution of the distances is more right-tailed, with most distances below 1.5. The model based on DropConnect was not able to produce complete clouds as consistently as the other models in terms of fidelity with the partial cloud. The histograms of average distances in Figure~\ref{fig:udhsairplane2} show the same kind of distributional patterns. The qualitative comparisons also validated this.
        \newline

        To quantify the approximation quality of the generated clouds with the ground truth complete clouds, the Earth mover's distance given by Eq~\ref{emd_loss} is computed between the complete cloud and each of the 10 generated clouds. In the first column of Figure~\ref{fig:emdsairplane}, histograms of all the values of Earth mover's distance combined across all test data from the airplane subcategory of the ShapeNet dataset are plotted for the different models. For the second column, the average of the Earth mover's distances between each ground truth complete cloud and the corresponding generated clouds was computed, and a histogram was plotted for average distances from the individual test inputs.
        \begin{figure}[htb]
          \centering
          \begin{subfigure}[t]{\dimexpr0.36\textwidth+20pt\relax}
            \makebox[20pt]{\raisebox{60pt}{\rotatebox[origin=c]{90}{\small DropCon}}}%
            \includegraphics[width=\dimexpr\linewidth-20pt\relax]{figures/dropcon/all_emds.png}
            \makebox[20pt]{\raisebox{60pt}{\rotatebox[origin=c]{90}{\small Dropout}}}%
            \includegraphics[width=\dimexpr\linewidth-20pt\relax]{figures/dropout/all_emds.png}
            \makebox[20pt]{\raisebox{60pt}{\rotatebox[origin=c]{90}{\small Ensemble}}}%
            \includegraphics[width=\dimexpr\linewidth-20pt\relax]{figures/ensemble/all_emds.png}
            \makebox[20pt]{\raisebox{60pt}{\rotatebox[origin=c]{90}{\small Implicit}}}%
            \includegraphics[width=\dimexpr\linewidth-20pt\relax]{figures/imle/all_emds.png}
            \caption{All distances}\label{fig:emdsairplane1}
          \end{subfigure}\hfill
          \begin{subfigure}[t]{0.36\textwidth}
            \includegraphics[width=\textwidth]{figures/dropcon/matched_mean_emds.png}
            \includegraphics[width=\textwidth]{figures/dropout/matched_mean_emds.png}
            \includegraphics[width=\textwidth]{figures/ensemble/matched_mean_emds.png}
            \includegraphics[width=\textwidth]{figures/imle/matched_mean_emds.png}
            \caption{Average distances}\label{fig:emdsairplane2}
          \end{subfigure}
          \caption{Quantitative comparison of approximation quality of the generated clouds between different empirical uncertainty quantification methods for point cloud completion on the airplane subcategory of ShapeNet data using Earth mover's distance.}
          \label{fig:emdsairplane}
        \end{figure}

        As observed in Figure~\label{fig:emdsairplane1}, most of the Earth mover's distances are below 0.075 for all methods except DropConnect. For DropConnect, the distribution of the distances is more right-tailed, with most distances below 0.125. The model based on DropConnect was not able to learn to approximate the ground truth complete clouds as well as the other models. The histograms of average distances in Figure~\ref{fig:emdsairplane2} show the same kind of distributional patterns. The qualitative comparisons also validated this.


\section{Evaluation of Implicit Representation-based Empirical Method}
    The original approach of learning to generate conditional implicit representations with randomness was not successful. There are multiple possible reasons behind this outcome. Firstly, due to resource and time constraints, it was not feasible to use the original architecture used in previous works for implicit representation learning from unoriented point clouds. Success of such methods depends significantly on precise architectures, parameter initialization, and hyperparameter tuning, which are quite costly. Even with proven initializations and hyperparameter values selected from previous works, the modified model was unable to learn the shape space conditioned on partial data.
    \\
    After investigating the learning process, it was found that the model overfits in a specific manner to minimize the overall loss without affecting the Eikonal loss or the Hessian term. The learned function basically predicts a small enough value everywhere so that the manifold loss is minimized, and the non-manifold loss is not too high. The Eikonal term is completely ignored, with the normal being close to zero everywhere. Consequently, the Hessian term also vanishes. This is evident from the Figure~\ref{fig:hessianfail} shown in Appendix~\ref{ch:additional_qual}. With further modification of the hyperparameters, this issue can be resolved, but it was not possible to implement this within the limited means of this work. Previous works~\cite{DiGS, NeuralHessian} show that even with complete cloud available, learning the implicit representations for the shape space is quite challenging, and the results are not satisfactory. Therefore, it is quite plausible that this method failed for this even more complex learning process, which involves conditioning on partial data and added randomness. 
    
    One simple alternative approach, as mentioned earlier, would be to learn the implicit representation for each generated complete shape from an input partial point cloud. Then, from the resulting implicit functions, an average surface can be empirically estimated, along with its associated uncertainty. Unfortunately, even with the state-of-the-art implicit representation learning methods for unoriented point clouds, the resulting reconstructed surfaces are spurious and not watertight (see Figure~\ref{fig:pccinr}). This was due to the fact that the completion methods were only able to produce 2048 points per complete cloud, which is often not dense enough for the implicit representation learning methods. An example of various reconstructed surfaces from different completed clouds for a randomly selected test data from the cabinet subcategory of the ShapeNet dataset is shown in Figure~\ref{fig:pccinr}. 

    \begin{figure}[htb]
        \centering
        \begin{subfigure}{0.19\textwidth}
            \includegraphics[width=\linewidth]{figures/inr_cabinet/part_cabi.png}
            \caption{Partial cloud}
        \end{subfigure}
        \hfill
        \begin{subfigure}{0.19\textwidth}
            \includegraphics[width=\linewidth]{figures/inr_cabinet/cabi_inr5.png}
            \caption{Recon 1}
        \end{subfigure}
        \hfill
        \begin{subfigure}{0.19\textwidth}
            \includegraphics[width=\linewidth]{figures/inr_cabinet/cabi_inr8.png}
            \caption{Recon 2}
        \end{subfigure}
        \hfill
        \begin{subfigure}{0.19\textwidth}
            \includegraphics[width=\linewidth]{figures/inr_cabinet/cabi_inr9.png}
            \caption{Recon 3}
        \end{subfigure}
        \hfill
        \begin{subfigure}{0.19\textwidth}
            \includegraphics[width=\linewidth]{figures/inr_cabinet/comp_cabi.png}
            \caption{Ground Truth}
        \end{subfigure}
        \hfill
        \caption{Surface reconstructions by learning multiple possible implicit representations from corresponding complete clouds generated for an input partial cloud. Recon $i$ denotes the $i$-th possible surface reconstructed from the generated complete cloud.}
        \label{fig:pccinr}
    \end{figure}


\section{Evaluation of Gaussian Process-based Method}
The results presented in this section are based solely on the implicit function described by a Gaussian process conditioned on partial cloud data, where contrastive learning-based feature embedding was employed prior to computing the GP posterior. Likelihood-based embedding learning was possible for smaller datasets where the dimensionality of the covariance matrix is limited to a certain extent. Some instances of this are presented in Figure~\ref{fig:2dgp} of Appendix~\ref{ch:additional_qual}. When dealing with larger point cloud data, numerical precision becomes a decisive factor in computing likelihood. Therefore, the contrastive learning-based priors were considered.
\\
Some examples of surface reconstruction from just a partial point cloud using contrastive learning-based feature embedding and a Gaussian process are shown in Figure~\ref{fig:gptable}. The GP posterior mean represents the surface (more specifically, the implicit function). The prior mean was set to 1. A $128^3$ sized grid was created around the input cloud, and the implicit function values were predicted using GP. The posterior means at those grid points were then used to perform marching cubes to obtain the reconstructed surface. It can be observed that contrastive learning-induced GP works quite well for surface reconstruction from partial clouds. However, the aspect of uncertainty quantification still remains an issue.   
\begin{figure}[htb]
  \centering
  \begin{subfigure}[t]{0.315\textwidth}
    \includegraphics[width=\textwidth]{figures/gp/t1pgp.png}
    \includegraphics[width=\textwidth]{figures/gp/t2pgp.png}
    \includegraphics[width=\textwidth]{figures/gp/t3pgp.png}
    \caption{Input}\label{fig:gptable1}
  \end{subfigure}\hfill
  \begin{subfigure}[t]{0.315\textwidth}
    \includegraphics[width=\textwidth]{figures/gp/t1gp.png}
    \includegraphics[width=\textwidth]{figures/gp/t2gp.png}
    \includegraphics[width=\textwidth]{figures/gp/t3gp.png}
    \caption{GP Posterior}\label{fig:gptable2}
  \end{subfigure}\hfill
  \begin{subfigure}[t]{0.315\textwidth}
    \includegraphics[width=\textwidth]{figures/gp/t1cgp.png}
    \includegraphics[width=\textwidth]{figures/gp/t2cgp.png}
    \includegraphics[width=\textwidth]{figures/gp/t3cgp.png}
    \caption{GT}\label{fig:gptable3}
  \end{subfigure}
  \caption{Surface reconstructions from implicit functions modeled by Gaussian process posterior mean from partial cloud inputs on randomly selected examples of the table subcategory of ShapeNet data.}
  \label{fig:gptable}
\end{figure}
\newline

Gaussian process was specifically adopted since it can provide a full stochastic formulation of our predictions. Unfortunately, the posterior covariance computation has an inherent problem in this setting. Since only the points from the partial cloud can be used for GP modeling in this task, the entries of the prior covariance matrix are based on those points, and no other supervision is available from non-manifold points. Contrastive learning maps the manifold points to similar embeddings; therefore, the entries in the covariance matrix are almost identical. This makes the matrix ill-conditioned, which causes numerical instability during inverse computation and therefore jeopardizes the posterior covariance computation.
\\
Even if the covariance matrix is sometimes non-singular or numerically stable, the variance at the vertices of the reconstructed mesh must be interpolated from the grid points where the GP posterior was computed. This leads to even more approximation and numerical errors, leading to non-existent or erroneous uncertainty values.