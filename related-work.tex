\chapter{Related Works}\label{ch:related-work}
Previous research that is relevant to our work can be roughly categorized into the following fields:



\section{Point Cloud-based Methods}\label{PCC-old}
Due to the ease of procuring, point clouds have been the most popular representation used for tasks related to 3D shapes. A similar trend was also observed for 3D shape completion tasks. On the other hand, with the increasing success of deep learning models in various domains, numerous works have also focused on utilizing deep neural networks for 3D shape learning tasks. Point cloud completion is also one of many such applications. Several previous works~\cite{PCN, PoinTr, PointAttN, VarPCN, PCNSkip, Snowflake} performed point cloud completion for a given partial point cloud by predicting the complete point cloud or the missing points in the cloud. 
\\
Point Completion Network (PCN)~\cite{PCN} was the first such approach that generated a complete cloud directly from the incomplete point cloud without using any voxelization techniques. PCN used an autoencoder (encoder-decoder) network to produce a denser point cloud as output in multiple stages. Other methods use variational autoencoder~\cite{VarPCN}, transformer-based encoder-decoder~\cite{PoinTr, Snowflake}, attention-based autoencoder~\cite{PointAttN, VarPCN, PCNSkip} for completion tasks. A complete review of all point cloud completion methods is not possible here. A comprehensive survey of such techniques can be found in~\cite{PCNSurvey}. 
\newline

However, the methods mentioned above are inherently deterministic with an injective mapping between partial and complete clouds, which is impractical for our purpose. Due to the inherent uncertainty of the completion process, one partial cloud can map to different complete shapes. Various works have approached this as a generative modeling task to incorporate the implicit ambiguity of the missing points. Generative Adversarial Networks (GANs)~\cite{GAN} have been the most popular generative modeling technique ever since it was introduced, and the same idea was also explored for point cloud completion in~\cite{GANPCC1, GANUPCC}. While the usage of GAN employed standard learning for partial and complete pairs in~\cite{GANPCC1}, the application was extended to unpaired point cloud completion in~\cite{GANUPCC}. However, those early works did not explore the idea of generating point clouds with uncertainty. 
\newline

For supervised point completion, generating multiple shapes from a single partial shape is difficult, since we only have one complete ground truth shape available for each partial input during training. Authors of~\cite{CGAN} addressed this issue by using a conditional generative adversarial network (c-GAN) along with a variational autoencoder (VAE) to learn to generate multiple complete clouds during inference. But because of mode collapse in GANs~\cite{ModeCollapseGAN}, generated shapes are often not geometrically diverse and differ only in simpler aspects such as length or width. Another approach~\cite{PCCIMLE} used implicit maximum likelihood estimation (IMLE) introduced in~\cite{IMLE} instead of GANs to incorporate multiple completions for an input partial shape without the downside of mode collapse. 
\\
Authors of~\cite{HyperPocket} predicted the missing parts of the shapes rather than generating complete shapes by enforcing the representation of the missing point cloud space to follow a predetermined probability distribution and using a hypernetwork~\cite{Hypernet} to train a decoder that takes the concatenated representations of the partial and missing points and outputs weights of the target network. The target network is trained to produce a complete point cloud from a simple probability distribution as implemented in~\cite{PCHypernet}. Another method~\cite{EBResLT} proposed incorporating an energy-based model (EBM)~\cite{EBM} to transport the partial cloud representation to the complete cloud representation using residual (the difference between partial and complete representations) in an unsupervised setting. Although these methods addressed the issue of the inherent non-determinism of point cloud completion, none except one~\cite{EBResLT} produced any output that can help quantify the uncertainty of such completions.


\section{Implicit Neural Representation-based Methods}\label{INR-old}
Another popular representation for 3D shapes used in recent works is an implicit function whose zero level set describes the underlying surface. Implicit function is memory-efficient and easy to evaluate once known, making it preferable over other representations such as voxels or point clouds. Consequently, numerous works have performed surface reconstruction from point clouds by recovering the implicit representation of the surface. Similar to point cloud completion, deep learning methods have also found considerable success in learning implicit representations of surfaces. We refer to these as implicit neural representations (INRs). Most of the existing works assume that normal information is available and use that to enforce some kind of geometric prior. These methods rely quite significantly on the normal information-based constraints to produce better results. But in our work, we are only interested in the works that predict an implicit representation from unoriented point clouds. These methods utilize different regularization techniques based on the available data. 
\newline

Sign agnostic learning introduced in~\cite{SAL} used the readily available unsigned distance values for points normally distributed around observed points by computing the distance to the nearest point in the point cloud, and used a differentiable sign-agnostic monotonic function as the loss function to recover the unsigned distance function. Sign agnostic learning with derivatives~\cite{SALD} extended this method to further include a term based on the gradients of the distance functions in the loss function and showed that the regularization based on gradients resulted in better learning. Implicit geometric regularization~\cite{IGR} enforced the gradients of the implicit function to have unit norm. Neural-Pull method~\cite{NeuralPull} sampled query points normally distributed around the ground truth point cloud and computed the pulled location of the query points based on the predicted distance function and its gradient to minimize the distance between the pulled locations and the closest point on the surface. Divergence-guided shape implicit neural representation~\cite{DiGS} minimized the magnitude of the divergence or Laplacian of the distance field as an added regularization. Neural Singular Hessian method~\cite{NeuralHessian}, on the other hand, constrained the Hessian matrix of the implicit function to have zero determinant for points on and close to the surface as a geometric prior, leading to state-of-the-art results for recovering INR from unoriented point clouds.


\section{Gaussian Process-based Methods}\label{Stoch-old}
Gaussian process is the most commonly used method for uncertainty quantification for regression tasks. Since learning the distance function of a 3D shape can be considered as a regression problem, many methods have tried to model the implicit function as a Gaussian process. The known distance function values act as observations of the GP and thus can be used to predict the distance values at other query points by computing the posterior distribution. The variance of the posterior distribution also quantifies the corresponding uncertainty of the predicted distance values. Such a representation of surfaces is referred to as Gaussian Process Implicit Surface (GPIS). GPIS was first introduced in~\cite{GPIS} where the supervision is done by fixing the distance values of points on the surface to zero and sampling virtual points inside and outside the surface with -1 and +1 distance values. Even with the discrete measurements, the surface can be recovered due to the interpolation abilities of GP regression. Kernels or covariance functions are an important part of GP regression that indicate the similarity between two points in shape and directly affect the posterior distribution. The authors of~\cite{GPIS} used a covariance function equivalent to the thin plate spline energy regularizer, which minimizes the overall curvature of the learned implicit function. 
\newline

Although distance field estimation using standard GPIS methods is accurate close to the surface, it does not hold as we move away from the surface due to the lack of training points, with the field converging to zero even far from the surface~\cite{logGPIS}. Inspired by the heat method proposed in ~\cite{GeodesicHeat}, authors of~\cite{logGPIS} suggested using the logarithm of standard GP regression to model the implicit surface termed as log-GPIS. But apart from log-GPIS not producing a true Euclidean distance field, it sacrifices interpolation abilities on the surface for accurate distance field estimation away from the surface~\cite{onlinePriorGPIS}. Moreover, the logarithmic transformation affects the precision of uncertainty quantification. Another work proposed an alternative formulation of the distance field as a reverse of a latent scalar field modelled by GP, where the reverting function corresponds to the inverse of the GP kernel~\cite{GPDF}.
\newline

In terms of performance, most of these methods suffer due to the cubic complexity of GP, and limiting the number of points to decrease computational cost might affect robustness. Several works~\cite{mixGPOccMap, locGPOccMap, onlineGPIS} have tried to solve the computational complexity issue by using multiple GPs, each modelled locally on partitioned clusters. Authors of~\cite{onlinePriorGPIS} used a distance field prior extracted from simpler geometric features on the GP mean function to reduce the model complexity. Another method~\cite{GMMGP} used a Gaussian Mixture Model (GMM) based prior instead of a geometrically extracted prior for the same purpose.
\newline

Stochastic Poisson Surface Reconstruction~\cite{SPSR} combined PSR with GP to formulate a stochastic version of PSR where the observed points along with the normal information in a point cloud are considered as observations of a GP. Therefore, a local GP is used to approximate the distribution of the gradient vector field. The GP posterior, in turn, gives us the mean and covariance functions of the vector field. Then we can recover the mean and covariance functions of the scalar field from those of the vector field using a global PDE solver~\cite{SPSR}. While in~\cite{SPSR} the authors used a standard PDE solver, in~\cite{NeuralSPSR} the authors replaced it with a neural PDE solver by parametrizing the mean and covariance of the implicit scalar field using an NN and optimizing it using gradient descent on losses defined from the variational version of the Poisson equation. This formulation allowed us even to extend to the screened version defined in~\cite{ScreenedPSR} by incorporating the screening terms in the loss function, as well as removing the complex discretization process needed for the standard PDE solvers~\cite{NeuralSPSR}. 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%